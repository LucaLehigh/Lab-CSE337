{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucaLehigh/Lab-CSE337/blob/main/lab5_approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 5: Value-Function Approximation\n",
        "\n",
        "\n",
        "## Exercise 1 SGD\n",
        "Many real-world processes can be modeled as nonlinear relationships, and SGD is a standard tool to fit those models from data.  \n",
        "\n",
        "**Example: Predicting energy consumption in a building**  \n",
        "- Energy usage depends on **temperature**, **time of day**, and **occupancy**.  \n",
        "- If you plot the data, the relationship might not be linear — it curves.  \n",
        "- A simple way to approximate this nonlinear relation is to fit a **polynomial function** of temperature (or time).  \n",
        "\n",
        "Now, imagine you’re collecting data continuously:  \n",
        "- You get one data point (temperature, usage) at a time.  \n",
        "- Instead of waiting to collect all data and computing a full batch update, you update your model incrementally with **SGD**.  \n",
        "- This makes your learning **online, adaptive, and scalable** — just like in reinforcement learning.  \n",
        "\n",
        "---\n",
        "\n",
        "In this exercise, approximating a cubic polynomial is a simplified version of **predicting a nonlinear real-world phenomenon**.\n"
      ],
      "metadata": {
        "id": "kXHxamYh1kD0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2lZYncHzhY-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic data based on a cubic polynomial with Gaussian noise\n",
        "np.random.seed(42)  # for reproducibility\n",
        "n_samples = 1000\n",
        "x = np.linspace(-5, 5, n_samples)\n",
        "# Define the true cubic polynomial function\n",
        "def true_function(x):\n",
        "  return 0.1 * x**3 - 0.5 * x**2 + 2 * x + 5\n",
        "\n",
        "y_true = true_function(x)\n",
        "noise = np.random.normal(0, 2, n_samples)  # Gaussian noise with mean 0 and std dev 2\n",
        "y = y_true + noise\n",
        "\n",
        "\n",
        "# Optional: Plot the generated data\n",
        "plt.scatter(x, y, label='Generated Data')\n",
        "plt.plot(x, y_true, color='red', label='True Function')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Generated Data with Cubic Polynomial and Gaussian Noise')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the given a dataset of input–output pairs `(x, y)` where the underlying relationship is approximately polynomial.  \n",
        "\n",
        "a. Use a **linear model with polynomial features**: Write a function that takes an input `x` and a weight vector `w`, and return the predicted value y_hat.    \n",
        "   \n",
        "   - y_hat(x; w) = w0 + w1 * x + w2 * x^2 + w3 * x^3  \n",
        "\n",
        "   where w0, w1, w2, w3 are the parameters to be learned.\n",
        "\n",
        "b. **Loss function (Mean Squared Error)**:\n",
        "   Write a function that computes the loss for one training point `(x, y)`:\n",
        "\n",
        "     - Formula: `loss = 0.5 * (y - y_hat)**2`\n",
        "\n",
        "c. **SGD update**:   Derive the gradient of the loss with respect to each parameter (w0, w1, w2, w3).  \n",
        "   - Write a function:  \n",
        "\n",
        "     ```python\n",
        "     def sgd_update(x, y, w, alpha):\n",
        "         \"\"\"\n",
        "         Perform one SGD update for a single training example.\n",
        "         Input:\n",
        "             x (float) - input value\n",
        "             y (float) - true output\n",
        "             w (np.array) - current weights\n",
        "             alpha (float) - learning rate\n",
        "         Output:\n",
        "             w (np.array) - updated weights\n",
        "         \"\"\"\n",
        "         # TODO: compute prediction, gradient, and update weights\n",
        "         return w\n",
        "     ```\n",
        "d. **Training loop**:\n",
        "   - Loop over the dataset.  \n",
        "   - At each step, update the weights using `sgd_update`.  \n",
        "   - Track the training loss after each iteration.  \n",
        "\n",
        "e. **Comparison with different learning rates**  \n",
        "   - Train your model using at least three different learning rates, for example:  \n",
        "     - alpha = 0.001  \n",
        "     - alpha = 0.01  \n",
        "     - alpha = 0.1  \n",
        "   - Plot training loss vs iteration for each learning rate.  \n",
        "   - Plot the final fitted polynomial curves for each learning rate on the same graph with the true dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### Deliverables\n",
        "- Code for `predict`, `mse_loss`, and `sgd_update`.  \n",
        "- Training loop that runs with multiple learning rates.  \n",
        "- Plot of training loss vs iteration for each learning rate.  \n",
        "- Plot of the fitted polynomial vs dataset for each learning rate.  \n",
        "- A short discussion:\n",
        "  - How does the learning rate affect convergence speed and stability?  \n",
        "  - Which learning rate gives the best balance between speed and accuracy?  "
      ],
      "metadata": {
        "id": "iTtVnZXS3c5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Your code here\n",
        "\n",
        "# Parameters\n",
        "alpha = 0.00001\n",
        "w = np.array([0.0, 0.0, 0.0, 0.0])\n",
        "loss_history = []\n",
        "\n",
        "# Predict function\n",
        "def predict(x, w):\n",
        "  y_hat = w[0] + (w[1] * x) + (w[2] * x**2) + (w[3] * x**3)\n",
        "  return y_hat\n",
        "\n",
        "# mse_loss\n",
        "def mse_loss(y, y_hat):\n",
        "  loss = 0.5 * (y - y_hat)**2\n",
        "  return loss\n",
        "\n",
        "#SGD Update\n",
        "def sgd_update(x, y, w, alpha):\n",
        "  y_hat = predict(x, w)\n",
        "  error = y - y_hat\n",
        "  w[0] += alpha * error\n",
        "  w[1] += alpha * error * x\n",
        "  w[2] += alpha * error * x**2\n",
        "  w[3] += alpha * error * x**3\n",
        "  return w\n",
        "\n",
        "#Training loop:\n",
        "\n",
        "#Loop over the dataset.\n",
        "#At each step, update the weights using sgd_update.\n",
        "#Track the training loss after each iteration.\n",
        "\n",
        "# Comparison with different learning rates\n",
        "loss_histories = {}\n",
        "alphas = [0.000001, 0.00001, 0.0001]\n",
        "for alpha in alphas:\n",
        "    w = np.array([0.0, 0.0, 0.0, 0.0])\n",
        "    loss_history = []\n",
        "    for i in range(n_samples):\n",
        "        w = sgd_update(x[i], y[i], w, alpha)\n",
        "        loss = mse_loss(y[i], predict(x[i], w))\n",
        "        loss_history.append(loss)\n",
        "    loss_histories[alpha] = (w, loss_history)\n",
        "\n",
        "# Plot training loss vs iteration for each learning rate.\n",
        "for alpha, (w, loss_history) in loss_histories.items():\n",
        "    plt.plot(loss_history, label=f'alpha={alpha}')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss vs Iteration')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 200)\n",
        "plt.show()\n",
        "\n",
        "# Plot the final fitted polynomial curves for each learning rate on the same graph with the true dataset.\n",
        "#plt.scatter(x, y, label='Generated Data')\n",
        "plt.plot(x, y_true, color='red', label='True Function')\n",
        "\n",
        "for alpha, (w, _) in loss_histories.items():\n",
        "    y_pred = predict(x, w)\n",
        "    plt.plot(x, y_pred, label=f'alpha={alpha}')\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.title('Fitted Polynomials for Different Learning Rates')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.ylim(-10,20)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "'''\n",
        "The smaller the learning rate the slower loss decreases but might take a while depending on the sample size. A larger learning rate can overshoot and oscillate but can converge with smaller samples.\n",
        "It seems that 0.0001 is the best learning rate but I am unable to make the learning rate higher than that. It seems that it has a good balance between speed and accuracy.\n",
        "'''"
      ],
      "metadata": {
        "id": "tWYv8xa3nGBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: 1000-State Random Walk with Semi-Gradient TD(0)\n",
        "\n",
        "We will study the 1,000-state Random Walk a classic reinforcement learning benchmark from Sutton & Barto.  \n",
        "\n",
        "- The environment has states numbered **1 to 1000**.  \n",
        "- There are two **terminal states**:  \n",
        "  - State `0` on the left (reward = 0)  \n",
        "  - State `1001` on the right (reward = 1)  \n",
        "- Each episode starts in the **middle** at state `500`.  \n",
        "- At each step, the agent moves **left or right with equal probability (0.5 each)**.  \n",
        "- The episode ends when the agent reaches either terminal.  \n",
        "- Discount factor: **γ = 1.0** (episodic task).  \n",
        "\n",
        "\n",
        "### Function Approximation\n",
        "Instead of storing a separate value for each state, approximate the value function with a **linear function of the state index**: V_hat(s; w0, w1) = w0 + w1 * s\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "1. **Implement the environment**:  \n",
        "   - You may **use AI tools such as ChatGPT** to generate the environment code (states, transitions, rewards).  \n",
        "   - Make sure you understand how the environment works.  \n",
        "\n",
        "2. **Implement the TD(0) update manually**:  \n",
        "   - Do **not** use AI for this part.  \n",
        "   - You must write the gradient update equations yourself using the formulas above.  \n",
        "\n",
        "3. **Train your agent**:  \n",
        "   - Run several episodes (e.g., 1000 episodes).  \n",
        "   - Experiment with different step sizes (`alpha`).  \n",
        "\n",
        "4. **Evaluate**:  \n",
        "   - Plot the **true value function** `V*(s) = s/1001`.  \n",
        "   - Plot your **learned approximation line** after training.  \n",
        "   - Discuss whether the line captures the overall trend of the true values.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Guidelines on Using AI Tools\n",
        "- You are encouraged to use AI tools (e.g., ChatGPT, Gemini, Copilot) to **generate helper code**, such as:  \n",
        "  - Building the random walk environment (`reset`, `step`).  \n",
        "  - Implementing the feature map φ(s).  \n",
        "  - Plotting results.  \n",
        "- However, **do not use AI tools to generate the TD(0) update equation**.  \n",
        "  - Deriving and implementing the update is the key learning objective of this exercise.  \n",
        "  - If we find code that uses an AI-generated update without understanding, the score will be zero.  \n",
        "\n",
        "\n",
        "## Deliverables\n",
        "- Python code for the environment and the TD(0) algorithm.  \n",
        "- Plot the **true value function**: For state `s`, the probability of reaching the right terminal is:  \n",
        "  `V*(s) = s / 1001`\n",
        "- Plot comparing the true value function and the approximated line.  \n",
        "- A short discussion:  \n",
        "  - How does the approximation behave for small vs large states?  \n",
        "  - How does the learning rate affect convergence?  \n",
        "\n",
        "---\n",
        "\n",
        "## Hints\n",
        "- Normalize states to `[0,1]` before using them in the line approximation to avoid very large values for w1.  \n",
        "- Start with small step sizes (e.g., 0.001–0.01).  \n",
        "- The approximation will not be perfect (a line cannot match the true curve), but should capture the increasing trend.  \n"
      ],
      "metadata": {
        "id": "n32oJJRFIWqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# Environment\n",
        "n_states = 1000\n",
        "start_state = 500\n",
        "gamma = 1.0\n",
        "\n",
        "def true_value(s):\n",
        "    return s / (n_states + 1)\n",
        "\n",
        "def phi(s):\n",
        "    return np.array([1.0, s / (n_states + 1)])  # normalized features\n",
        "\n",
        "def run_td(alpha, n_episodes=2000, max_steps=5000):\n",
        "    w = np.zeros(2)\n",
        "    for ep in range(n_episodes):\n",
        "        s = start_state\n",
        "        steps = 0\n",
        "        while 1 <= s <= n_states and steps < max_steps:\n",
        "            steps += 1\n",
        "            s_next = s + (1 if np.random.rand() < 0.5 else -1)\n",
        "\n",
        "            r = 1.0 if s_next == n_states + 1 else 0.0\n",
        "\n",
        "            v_s = np.dot(w, phi(s))\n",
        "            v_next = 0.0 if s_next <= 0 or s_next > n_states else np.dot(w, phi(s_next))\n",
        "\n",
        "            delta = r + gamma * v_next - v_s\n",
        "            w += alpha * delta * phi(s)\n",
        "\n",
        "            s = s_next\n",
        "    return w\n",
        "\n",
        "# Train with different learning rates\n",
        "alphas = [0.001, 0.005, 0.01]\n",
        "weights = {alpha: run_td(alpha) for alpha in alphas}\n",
        "\n",
        "# Plot true vs approximations\n",
        "s = np.arange(1, n_states + 1)\n",
        "plt.plot(s, true_value(s), label=\"True V*(s)\", color=\"red\")\n",
        "for alpha, w in weights.items():\n",
        "    v_hat = w[0] + w[1] * (s / (n_states + 1))\n",
        "    plt.plot(s, v_hat, label=f\"alpha={alpha}, w={np.round(w,3)}\")\n",
        "plt.xlabel(\"State s\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"True vs Approximated Value Function\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "for alpha, w in weights.items():\n",
        "    print(f\"alpha={alpha} -> w0={w[0]:.4f}, w1={w[1]:.4f}\")\n"
      ],
      "metadata": {
        "id": "zHehiFTNKyxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Solving MountainCar with Tile Coding and SARSA\n",
        "\n",
        "## Problem Description\n",
        "In this exercise, you will solve the **MountainCar-v0** environment from Gym using **tile coding** for function approximation and the **SARSA algorithm** for learning.  \n",
        "\n",
        "The **Mountain Car problem**:\n",
        "- A car is stuck in a valley and is too weak to drive straight up to the goal.  \n",
        "- It must build momentum by going back and forth until it can reach the goal at `position >= 0.5`.  \n",
        "- **State space**: continuous (position, velocity).  \n",
        "- **Actions**: {0: push left, 1: no push, 2: push right}.  \n",
        "- **Reward**: -1 per step until the goal is reached.  \n",
        "- **Episode ends**: when the car reaches the goal or after 200 steps.  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 1: Environment and Random Policy (with AI help)\n",
        "- Use an **AI tool (e.g., ChatGPT)** to generate starter code for:\n",
        "  - Creating the Gym environment (`MountainCar-v0`).  \n",
        "  - Running a **random policy** (actions chosen randomly).  \n",
        "- Run this code to confirm you can interact with the environment and see episode returns.  \n",
        "- This will serve as a **baseline**.  \n",
        "- **Important**: Do not use AI to implement the learning algorithm.  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 2: SARSA Algorithm with Function Approximation\n",
        "Implement **SARSA (on-policy TD control)** with the following steps:\n",
        "\n",
        "For each episode:\n",
        "1. Initialize state `s`.  \n",
        "2. Choose action `a` using **ε-greedy** based on Q(s,a).  \n",
        "3. For each step:  \n",
        "   - Take action `a`, observe `(s_next, r, done)`.  \n",
        "   - Choose next action `a_next` using ε-greedy from `s_next`.  \n",
        "   - Compute TD target:  \n",
        "     ```\n",
        "     target = r + gamma * Q(s_next, a_next)\n",
        "     ```  \n",
        "     (if `s_next` is terminal, then target = r).  \n",
        "   - Compute TD error:  \n",
        "     ```\n",
        "     delta = target - Q(s,a)\n",
        "     ```  \n",
        "   - Update weights:  \n",
        "     ```\n",
        "     w <- w + alpha * delta * x(s,a)\n",
        "     ```  \n",
        "   - Update `s = s_next`, `a = a_next`.  \n",
        "4. End episode when the goal is reached or step limit is hit.  \n",
        "\n",
        "---\n",
        "\n",
        "## Step 3: Experiments\n",
        "- Train the agent for 500–1000 episodes.  \n",
        "- Plot **episode returns (sum of rewards)** vs episodes.  \n",
        "- Compare with the random policy baseline:  \n",
        "  - Does SARSA learn to consistently reach the goal?  \n",
        "  - How many steps does it typically take?  \n",
        "\n",
        "---\n",
        "\n",
        "## Deliverables\n",
        "- Python code for tile coding and SARSA.  \n",
        "- Plot of returns vs episodes.  \n",
        "- Plot the Value function\n",
        "- Short discussion (1–2 paragraphs):  \n",
        "  - Effect of tile coding parameters (number of tilings, resolution).\n"
      ],
      "metadata": {
        "id": "X8ooMF71OBCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can use the following code for tiling\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TileCoderXY:\n",
        "    \"\"\"\n",
        "    A TileCoder for function approximation that applies tile coding on the x and y coordinates\n",
        "    of a 3D state. Instead of providing tile widths, the user provides the number of tiles per\n",
        "    dimension. The tile widths are computed based on the state bounds and the number of tiles.\n",
        "    The z coordinate is not used.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_tilings, tiles_per_dim, state_low, state_high):\n",
        "        \"\"\"\n",
        "        Initialize the TileCoderXY.\n",
        "\n",
        "        Parameters:\n",
        "            num_tilings (int): Number of overlapping tilings.\n",
        "            tiles_per_dim (array-like of 2 ints): Number of tiles along the x and y dimensions.\n",
        "            state_low (array-like of 2 floats): Lower bounds for the x and y dimensions.\n",
        "            state_high (array-like of 2 floats): Upper bounds for the x and y dimensions.\n",
        "        \"\"\"\n",
        "        self.num_tilings = num_tilings\n",
        "        self.tiles_per_dim = np.array(tiles_per_dim, dtype=int)\n",
        "        self.state_low = np.array(state_low, dtype=float)\n",
        "        self.state_high = np.array(state_high, dtype=float)\n",
        "\n",
        "        # Compute the tile width for each dimension.\n",
        "        # We assume that the grid spans exactly from state_low to state_high.\n",
        "        # When there are N tiles, there are N-1 intervals between the boundaries.\n",
        "        self.tile_width = (self.state_high - self.state_low) / (self.tiles_per_dim - 1)\n",
        "\n",
        "        # Precompute an offset for each tiling to create overlapping grids.\n",
        "        # self.offsets = [(i / self.num_tilings) * self.tile_width for i in range(self.num_tilings)]\n",
        "        # self.offsets = self.compute_8_offsets()\n",
        "        # self.offsets = np.stack(self._compute_offsets(), axis=0)  # shape: (num_tilings, dims)\n",
        "        # Precompute offsets for each tiling.\n",
        "        # For tiling i:\n",
        "        #   offset_x = (((i + 0) % num_tilings) / num_tilings) * tile_width[0]\n",
        "        #   offset_y = (((i + 1) % num_tilings) / num_tilings) * tile_width[1]\n",
        "        offsets = np.empty((self.num_tilings, 2))\n",
        "        for i in range(self.num_tilings):\n",
        "            offsets[i, 0] = (((i + 0) % self.num_tilings) / self.num_tilings) * self.tile_width[0]\n",
        "            offsets[i, 1] = (((i + 1) % self.num_tilings) / self.num_tilings) * self.tile_width[1]\n",
        "        self.offsets = offsets\n",
        "\n",
        "\n",
        "        # Precompute multiplier for flattening a 2D index.\n",
        "        # For grid shape (N, M), flat index = x_index * M + y_index.\n",
        "        self.multiplier = self.tiles_per_dim[1]\n",
        "\n",
        "        # Initialize a weight vector for each tiling.\n",
        "        num_tiles = np.prod(self.tiles_per_dim)\n",
        "        self.weights = [np.zeros(num_tiles) for _ in range(self.num_tilings)]\n",
        "\n",
        "    def save(self, file_name):\n",
        "        np.savez(file_name + \".npz\", weights=self.weights)\n",
        "\n",
        "    def load(self, file_name):\n",
        "        self.weights = np.load(file_name+\".npz\")[\"weights\"]\n",
        "\n",
        "\n",
        "    def compute_8_offsets(self):\n",
        "        \"\"\"\n",
        "        Compute a list of offsets using a combination of cardinal and diagonal directions.\n",
        "        The offsets include:\n",
        "          - Center: [0, 0]\n",
        "          - Cardinal: right, left, up, down (half-tile shifts)\n",
        "          - Diagonal: up-right, up-left, down-right, down-left (half-tile shifts)\n",
        "\n",
        "        If the number of tilings exceeds the number of unique offsets (9), the list is repeated.\n",
        "\n",
        "        Returns:\n",
        "            List of 2-element numpy arrays representing the offset for each tiling.\n",
        "        \"\"\"\n",
        "        half_tile = self.tile_width / 8.0\n",
        "        base_offsets = [\n",
        "            np.array([0.0, 0.0]),  # Center (no shift)\n",
        "            np.array([half_tile[0], 0.0]),  # Right\n",
        "            np.array([-half_tile[0], 0.0]),  # Left\n",
        "            np.array([0.0, half_tile[1]]),  # Up\n",
        "            np.array([0.0, -half_tile[1]]),  # Down\n",
        "            np.array([half_tile[0], half_tile[1]]),  # Up-right\n",
        "            np.array([-half_tile[0], half_tile[1]]),  # Up-left\n",
        "            np.array([half_tile[0], -half_tile[1]]),  # Down-right\n",
        "            np.array([-half_tile[0], -half_tile[1]])  # Down-left\n",
        "        ]\n",
        "        offsets = []\n",
        "        for i in range(self.num_tilings):\n",
        "            offsets.append(base_offsets[i % len(base_offsets)])\n",
        "        return offsets\n",
        "\n",
        "    def get_tile_indices(self, state):\n",
        "        \"\"\"\n",
        "        Compute the active tile indices for all tilings given a 2D state.\n",
        "\n",
        "        Parameters:\n",
        "            state (array-like of length 2): The input state [x, y].\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (tiling_index, flat_tile_index) for each tiling.\n",
        "        \"\"\"\n",
        "        state = np.array(state, dtype=float)  # shape: (2,)\n",
        "        # Compute shifted states for all tilings in one vectorized operation.\n",
        "        # Shape of shifted: (num_tilings, 2)\n",
        "        shifted = (state - self.state_low) + self.offsets\n",
        "\n",
        "        # Compute tile coordinates (integer indices) for each tiling.\n",
        "        # Division is broadcasted over the offsets.\n",
        "        tile_coords = (shifted / self.tile_width).astype(int)  # shape: (num_tilings, 2)\n",
        "\n",
        "        # Clip to ensure indices are within bounds.\n",
        "        tile_coords[:, 0] = np.clip(tile_coords[:, 0], 0, self.tiles_per_dim[0] - 1)\n",
        "        tile_coords[:, 1] = np.clip(tile_coords[:, 1], 0, self.tiles_per_dim[1] - 1)\n",
        "\n",
        "        # Compute flat indices for each tiling.\n",
        "        # flat_index = x_index * (tiles_per_dim[1]) + y_index\n",
        "        flat_indices = tile_coords[:, 0] * self.tiles_per_dim[1] + tile_coords[:, 1]\n",
        "\n",
        "        # Return a list of (tiling_index, flat_index) tuples.\n",
        "        return list(zip(range(self.num_tilings), flat_indices))\n",
        "\n",
        "\n",
        "    def predict(self, state):\n",
        "        \"\"\"\n",
        "        Compute the approximated function value for a given 3D state using tile coding on x and y.\n",
        "\n",
        "        Parameters:\n",
        "            state (array-like): The input state [x, y, z].\n",
        "\n",
        "        Returns:\n",
        "            float: The function approximation (sum of weights for the active tiles).\n",
        "        \"\"\"\n",
        "        active_tiles = self.get_tile_indices(state)\n",
        "        return sum(self.weights[tiling][idx] for tiling, idx in active_tiles)\n",
        "\n",
        "    def update(self, state, target, alpha):\n",
        "        \"\"\"\n",
        "        Update the weights given a state and target value.\n",
        "\n",
        "        Parameters:\n",
        "            state (array-like): The input state [x, y, z].\n",
        "            target (float): The target function value.\n",
        "            alpha (float): The overall learning rate.\n",
        "        \"\"\"\n",
        "        prediction = self.predict(state)\n",
        "        error = target - prediction\n",
        "        # Distribute the learning rate equally among all tilings.\n",
        "        alpha_per_tiling = alpha / self.num_tilings\n",
        "\n",
        "        active_tiles = self.get_tile_indices(state)\n",
        "        for tiling, idx in active_tiles:\n",
        "            self.weights[tiling][idx] += alpha_per_tiling * error\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UWX5U5JtOO1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward is -1 for every step"
      ],
      "metadata": {
        "id": "7zkISiB3JRab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}